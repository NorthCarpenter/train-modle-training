{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "import os\n",
    "import csv\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Code                   Name\n",
      "0    ABE           Aberdeen, MD\n",
      "1    ABQ        Albuquerque, NM\n",
      "2    ACA  Antioch-Pittsburg, CA\n",
      "3    ACD            Arcadia, MO\n",
      "4    ADM            Ardmore, OK\n",
      "..   ...                    ...\n",
      "552  XXX                    NaN\n",
      "553  YAZ         Yazoo City, MS\n",
      "554  YEM           Yemassee, SC\n",
      "555  YNY            Yonkers, NY\n",
      "556  YUM               Yuma, AZ\n",
      "\n",
      "[557 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "557"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ========= Scraping and Analysing Amtrack Delay Times =========\n",
    "\n",
    "# Scraping\n",
    "# Collecting Amtrack station IDs\n",
    "\n",
    "id_url = \"https://juckins.net/amtrak_status/archive/html/stations.php\"\n",
    "req_id = urllib.request.Request(id_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "id_html = urllib.request.urlopen(req_id).read()\n",
    "\n",
    "station_ids = pd.read_html(id_html, header = 1)[0]\n",
    "\n",
    "print(station_ids)\n",
    "len(station_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6684"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Translating station Codes into a list of usable URLs\n",
    "\n",
    "station_urls = {}\n",
    "years_to_use = [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021]\n",
    "\n",
    "generic_url = \"https://juckins.net/amtrak_status/archive/html/history.php?train_num=all&station={station}&date_start=01%2F01%2F{low_year}&date_end=12%2F31%2F{high_year}&df1=1&df2=1&df3=1&df4=1&df5=1&df6=1&df7=1&sort=schDp&sort_dir=DESC&co=gt&limit_mins=&dfon=1\"\n",
    "\n",
    "station_code = station_ids['Code'].tolist()\n",
    "\n",
    "for Code in station_code:\n",
    "    for year in years_to_use:\n",
    "        station_url = generic_url.format(station = Code, low_year = str(year), high_year = str(year + 1))\n",
    "        url_Name = Code + \"_\" + str(year) + \"-\" + str(year + 1)\n",
    "        station_urls.update({url_Name : station_url})\n",
    "\n",
    "len(station_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# Scraping All Data Pages\n",
    "\n",
    "# Making a dictionary we can strip values out of\n",
    "stationDic = station_urls.copy()\n",
    "\n",
    "# Getting a list of already downloaded files\n",
    "collectedTrainData = os.listdir(path='/Users/NorthCarpenter/Documents/Sort/Employment/Making Employment/Train Code/trainDataStation')\n",
    "collectedTrainData.remove(\"noDataStatYr.csv\")\n",
    "collectedTrainData.remove(\".DS_Store\")\n",
    "\n",
    "# Getting the list of files without data\n",
    "with open('/Users/NorthCarpenter/Documents/Sort/Employment/Making Employment/Train Code/trainDataStation/noDataStatYr.csv', 'r', newline = '') as noDatCSV:\n",
    "    reader = csv.reader(noDatCSV)\n",
    "    noDataList = list(reader)\n",
    "\n",
    "# Remove URL's with no data or already downloaded\n",
    "\n",
    "if len(collectedTrainData) >= 1:\n",
    "    for stationYear in collectedTrainData:\n",
    "        stationDic.pop(stationYear[:-4])\n",
    "\n",
    "if len(noDataList) >= 1:\n",
    "    for stationYear in noDataList:\n",
    "        stationDic.pop(stationYear[0])\n",
    "\n",
    "# Set Path of folder to save data\n",
    "pathTrainFolder = os.path.abspath('/Users/NorthCarpenter/Documents/Sort/Employment/Making Employment/Train Code/trainDataStation')\n",
    "\n",
    "\n",
    "for url in tqdm.tqdm(stationDic):\n",
    "    try:\n",
    "        req_station = urllib.request.Request(stationDic[url], headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        station_html = urllib.request.urlopen(req_station).read()\n",
    "        station_data = pd.read_html(station_html, header = 1)\n",
    "        station_data = station_data[0]\n",
    "        outputPath = os.path.join(pathTrainFolder, url + '.csv')\n",
    "        station_data.to_csv(outputPath, index = False)\n",
    "        print(url)\n",
    "    except:\n",
    "        with open('/Users/NorthCarpenter/Documents/Sort/Employment/Making Employment/Train Code/trainDataStation/noDataStatYr.csv', 'a', newline='') as csv_file:\n",
    "            csv_file.write(url)\n",
    "            csv_file.write('\\n')\n",
    "        print(url, \" FAIL\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
